# ðŸš— The Spiraloid AI Driver's Test

A self-assessment tool that evaluates how effectively you use Large Language Models (LLMs). Paste it into your AI chat and receive an instant skill score based on your conversation history.

## What is this?

This is a specialized evaluation prompt that analyzes your past interactions with AI models to assess your LLM usage proficiency. Think of it as a "driver's test" for prompt engineering and AI collaboration skills.

The test evaluates **9 key dimensions** of LLM mastery:

1. ðŸ› ï¸ **Operational Mastery** - How you structure prompts and maintain coherent workflows
2. ðŸ” **Evaluation Depth** - Your ability to critically assess AI outputs
3. ðŸ§  **Mental Model Alignment** - Understanding how LLMs actually work
4. ðŸ—£ï¸ **Linguistic Control** - Precision and efficiency in your instructions
5. ðŸ§  **Context Window Awareness** - Managing the model's memory constraints
6. ðŸŽ¯ **Context Planning Precision** - Strategic information architecture
7. ðŸŽ· **Linguistic Payload Capacity** - Expressive range and stylistic control (the "Jazz" dimension)
8. ðŸŽ² **Collaborative Discovery** - Using AI as an exploration partner, not just an execution engine
9. ðŸªž **Echo Chamber Resistance** - Avoiding sycophancy and maintaining epistemic hygiene

## How it works

1. The test is pasted into a new conversation with an LLM
2. The AI analyzes your **prior conversation history** (not the test itself)
3. You receive:
   - A score from 1-10
   - A tier classification
   - Specific feedback on what you're doing well/poorly
   - Concrete suggestions for improvement
   - A confidence rating based on available evidence

The evaluation uses a **contradiction-based scoring system**: each clear violation of best practices counts against your score. It's deliberately strictâ€”favoring observed behavior over intent.

## How to use

### Quick Start

1. Copy the entire contents of `The Spiraloid AI Driver's Test.md`
2. Open a new conversation with your preferred LLM (ChatGPT, Claude, Gemini, etc.)
3. Paste the full test document
4. Receive your evaluation

**Important:** The test evaluates your **existing conversation history** with that model. For best results, use it in an account where you have substantial prior interactions.

### Privacy

ðŸ”’ **100% Private**: This evaluation runs entirely within your own chat session. No data is sent to external services. The AI only analyzes what's already in your conversation history.

## What you'll learn

The test reveals patterns you might not notice:

- Whether you're **over-explaining** or being appropriately concise
- If you're treating the AI as a **tool** vs. a conversational partner
- How well you **challenge and verify** AI outputs
- Whether you're **exploring possibilities** or just executing known patterns
- If you're vulnerable to **AI sycophancy** (when the AI just agrees with you)
- How effectively you use **linguistic nuance** to shape outputs

## Scoring tiers

- **10** â€” Master-level control. Prompts are scoped, compressed, anticipatory, and robust
- **8â€“9** â€” Highly competent. Minor inefficiencies or isolated contradictions
- **6â€“7** â€” Intermediate. Conceptual understanding with inconsistent execution
- **4â€“5** â€” Basic. Functional requests with weak structure or planning
- **1â€“3** â€” Novice. Vague, verbose, repetitive, or structurally unstable usage

## Why this matters

Most people use LLMs at 20% of their potential. They:

- Treat AI as a search engine instead of a collaborative tool
- Accept outputs without critical evaluation
- Use the same flat prompting style for every task
- Miss the difference between execution mode and discovery mode

This test helps you identify your gaps and provides **actionable techniques** to level up.

## Example insights you might receive

> "You tend to keep 'training wheels' in your prompts even after the AI understands what you wantâ€”removing them earlier frees up space and often produces cleaner output."

> "You're not using the AI for discovery. Try 'Show me 5 directions, I'll steer from there' instead of specifying everything upfront."

> "The AI is mirroring your framing back at you. Prompt adversariallyâ€”ask 'What's wrong with this idea?' and you'll get actual thinking instead of agreement."

## Advanced usage

After receiving your initial score, you can ask for:

- More detailed feedback on specific dimensions
- Concrete corrective techniques
- Before/after prompt examples
- Explanations of why certain patterns reduce model performance

## Who created this?

This test was developed by a Context Engineer and Swarm coder name Bay Raitt (spiraloid) to address a gap in AI literacy: most users don't know what "good" LLM usage looks like. It's designed to be:

- **Self-contained**: No sign-ups, no data collection
- **Educational**: Teaches by showing gaps, not just scoring
- **Practical**: Focuses on techniques you can apply immediately

## Contributing

Found a bug or have suggestions for improvement? Feel free to open an issue or submit a pull request.

## License

This project is open source. Use it, share it, modify it. The goal is to help people use AI more effectively.

---

**Note:** The test works best with models that have substantial conversation history to analyze. If you're testing it in a brand new account, the confidence rating will be low and results may be limited.
